{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T09:12:09.827566Z",
     "start_time": "2024-12-20T09:11:54.711600Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from model_setup import get_model\n",
    "from data import load_and_preprocess_data"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T09:12:09.837012Z",
     "start_time": "2024-12-20T09:12:09.833572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 全局变量\n",
    "device = torch.device(\"cpu\")\n",
    "model_path = \"./models/test/\"  # 模型保存路径\n",
    "data_path = \"./data/save\"  # 数据保存路径"
   ],
   "id": "3750f160897d5e9c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T09:12:09.877661Z",
     "start_time": "2024-12-20T09:12:09.872393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_model(model_name, label_flag, input_channels=1):\n",
    "    print(f\"Inside load_model: {model_name}, label_flag={label_flag}\")\n",
    "\n",
    "    num_classes_map = {0: 10, 1: 26, 2: 36, 3: 62}\n",
    "    if label_flag not in num_classes_map:\n",
    "        raise KeyError(f\"Invalid label_flag: {label_flag}. Supported values are {list(num_classes_map.keys())}\")\n",
    "    num_classes = num_classes_map[label_flag]\n",
    "\n",
    "    model = get_model(model_name, label_flag, input_channels=input_channels, num_classes=num_classes)\n",
    "    model_file = f\"{model_path}/{model_name}_{label_flag}.pt\"\n",
    "\n",
    "    print(f\"Attempting to load model from: {model_file}\")\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n",
    "        print(f\"Model {model_name} loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file not found: {model_file}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n"
   ],
   "id": "c83176b840fa32fe",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T09:12:09.892944Z",
     "start_time": "2024-12-20T09:12:09.883946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_confusion_matrix(model, val_loader, num_classes):\n",
    "    \"\"\"生成混淆矩阵，支持动态分类数\"\"\"\n",
    "    val_preds, val_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_targets.extend(labels.numpy())\n",
    "\n",
    "    # 动态生成混淆矩阵\n",
    "    cm = confusion_matrix(val_targets, val_preds, labels=range(num_classes))\n",
    "    return cm\n",
    "\n",
    "def visualize_feature_distribution(model, val_loader):\n",
    "    \"\"\"生成 t-SNE 特征分布\"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            features = model.conv_layers(inputs)  # 提取卷积层特征\n",
    "            all_features.append(features.cpu().view(features.size(0), -1).numpy())\n",
    "            all_labels.append(labels.numpy())\n",
    "    all_features = np.concatenate(all_features, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    features_2d = tsne.fit_transform(all_features)\n",
    "    return features_2d, all_labels\n",
    "\n",
    "\n",
    "def visualize_samples(val_loader, model, correct=True, num_samples=6):\n",
    "    \"\"\"生成分类样本的可视化子图\"\"\"\n",
    "    val_preds, val_targets, val_inputs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_targets.extend(labels.numpy())\n",
    "            val_inputs.extend(inputs.cpu().numpy())\n",
    "    val_inputs = np.array(val_inputs)\n",
    "    indices = [i for i, (p, t) in enumerate(zip(val_preds, val_targets)) if (p == t) == correct]\n",
    "    indices = indices[:num_samples]\n",
    "    return [(val_inputs[i, 0], val_targets[i], val_preds[i]) for i in indices]"
   ],
   "id": "6cdd4cec3aaa0ca3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T09:12:09.916645Z",
     "start_time": "2024-12-20T09:12:09.905667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_models_to_pdfs(data_path, epoch_number=20):\n",
    "    \"\"\"处理所有模型并生成多个PDF文件\"\"\"\n",
    "    files = [f for f in os.listdir(data_path) if f.endswith(\".xlsx\")]\n",
    "    print(\"Files in data_path:\", files)  # 打印文件列表以调试\n",
    "\n",
    "    processed_models = []\n",
    "    skipped_files = []\n",
    "\n",
    "    # 初始化PDF文件\n",
    "    confusion_matrix_pdf = PdfPages(\"confusion_matrices.pdf\")\n",
    "    # tsne_pdf = PdfPages(\"tsne_feature_distributions.pdf\")\n",
    "    incorrect_samples_pdf = PdfPages(\"incorrect_samples.pdf\")\n",
    "    correct_samples_pdf = PdfPages(\"correct_samples.pdf\")\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            # 解析模型名称和 label_flag\n",
    "            model_name, label_flag = file.split(\"_label_\")[0], int(file.split(\"_label_\")[1].split(\".\")[0])\n",
    "            print(f\"Processing file: {file}, Parsed model_name: {model_name}, label_flag: {label_flag}\")\n",
    "\n",
    "            num_classes_map = {0: 10, 1: 26, 2: 36, 3: 62}\n",
    "            if label_flag not in num_classes_map:\n",
    "                print(f\"Skipping unsupported label_flag: {label_flag}\")\n",
    "                skipped_files.append((file, f\"Unsupported label_flag: {label_flag}\"))\n",
    "                continue\n",
    "\n",
    "            num_classes = num_classes_map[label_flag]\n",
    "\n",
    "            # 加载数据\n",
    "            x_min_max, x_mean, y, kf_splits = load_and_preprocess_data(k=10, label_deal_flag=label_flag)\n",
    "            train_idx, val_idx = kf_splits[0]\n",
    "            x_val = x_min_max[val_idx]\n",
    "            y_val = y[val_idx]\n",
    "            x_val_tensor = torch.tensor(x_val).float().unsqueeze(1)\n",
    "            y_val_tensor = torch.tensor(y_val).long()\n",
    "            val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "            # 加载模型\n",
    "            model = load_model(model_name, label_flag)\n",
    "            print(f\"Model loaded: {model_name}, label_flag={label_flag}\")\n",
    "\n",
    "            processed_models.append((model_name, label_flag))\n",
    "\n",
    "            # 混淆矩阵\n",
    "            print(f\"Generating confusion matrix for model: {model_name}, label_flag: {label_flag}\")\n",
    "            cm = visualize_confusion_matrix(model, val_loader, num_classes=num_classes)\n",
    "\n",
    "            # 动态调整图像尺寸\n",
    "            figsize = (num_classes // 2 + 5, num_classes // 2 + 5)  # 确保图像足够大\n",
    "            fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "            # 创建混淆矩阵显示对象，并取消点上的数字显示\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(num_classes))\n",
    "            disp.plot(ax=ax, cmap=\"Blues\", colorbar=True, include_values=False)\n",
    "\n",
    "            # 调整坐标轴，避免标签重叠\n",
    "            ax.set_xticks(range(0, num_classes, max(1, num_classes // 10)))  # 每隔一定数量显示一个标签\n",
    "            ax.set_xticklabels(range(0, num_classes, max(1, num_classes // 10)), rotation=45, ha=\"right\", fontsize=8)\n",
    "            ax.set_yticks(range(0, num_classes, max(1, num_classes // 10)))  # 同样调整 y 轴\n",
    "            ax.set_yticklabels(range(0, num_classes, max(1, num_classes // 10)), fontsize=8)\n",
    "\n",
    "            # 设置标题\n",
    "            ax.set_title(f\"Confusion Matrix\\nModel: {model_name}, Label: {label_flag}\")\n",
    "            confusion_matrix_pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "            # # 特征分布图（t-SNE）\n",
    "            # print(f\"Generating t-SNE for model: {model_name}, label_flag: {label_flag}\")\n",
    "            # features_2d, labels = visualize_feature_distribution(model, val_loader)\n",
    "            # fig_tsne, ax_tsne = plt.subplots(figsize=(8, 8))\n",
    "            # scatter = ax_tsne.scatter(features_2d[:, 0], features_2d[:, 1], c=labels, cmap=\"viridis\", s=5)\n",
    "            # ax_tsne.set_title(f\"t-SNE Distribution\\nModel: {model_name}, Label: {label_flag}\")\n",
    "            # tsne_pdf.savefig(fig_tsne)\n",
    "            # plt.close(fig_tsne)\n",
    "\n",
    "            # 错误分类样本\n",
    "            print(f\"Generating incorrect samples for model: {model_name}, label_flag: {label_flag}\")\n",
    "            incorrect_samples = visualize_samples(val_loader, model, correct=False, num_samples=25)\n",
    "            fig_incorrect, axs_incorrect = plt.subplots(5, 5, figsize=(16, 16))  # 5x5布局\n",
    "            for i, (img, true_label, pred_label) in enumerate(incorrect_samples):\n",
    "                ax = axs_incorrect[i // 5, i % 5]\n",
    "                ax.imshow(img, cmap=\"gray\")\n",
    "                ax.imshow(img.T, cmap=\"gray\")  # 增加转置\n",
    "                ax.set_title(f\"T: {true_label}\\nP: {pred_label}\", fontsize=8)\n",
    "                ax.axis(\"off\")\n",
    "            fig_incorrect.suptitle(f\"Incorrect Samples\\nModel: {model_name}, Label: {label_flag}\")\n",
    "            incorrect_samples_pdf.savefig(fig_incorrect)\n",
    "            plt.close(fig_incorrect)\n",
    "\n",
    "            # 正确分类样本\n",
    "            print(f\"Generating correct samples for model: {model_name}, label_flag: {label_flag}\")\n",
    "            correct_samples = visualize_samples(val_loader, model, correct=True, num_samples=25)\n",
    "            fig_correct, axs_correct = plt.subplots(5, 5, figsize=(16, 16))  # 5x5布局\n",
    "            for i, (img, true_label, pred_label) in enumerate(correct_samples):\n",
    "                ax = axs_correct[i // 5, i % 5]\n",
    "                ax.imshow(img, cmap=\"gray\")\n",
    "                ax.imshow(img.T, cmap=\"gray\")  # 增加转置\n",
    "                ax.set_title(f\"T: {true_label}\\nP: {pred_label}\", fontsize=8)\n",
    "                ax.axis(\"off\")\n",
    "            fig_correct.suptitle(f\"Correct Samples\\nModel: {model_name}, Label: {label_flag}\")\n",
    "            correct_samples_pdf.savefig(fig_correct)\n",
    "            plt.close(fig_correct)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "            skipped_files.append((file, f\"Error: {e}\"))\n",
    "            continue\n",
    "\n",
    "    # 关闭PDF文件\n",
    "    confusion_matrix_pdf.close()\n",
    "    # tsne_pdf.close()\n",
    "    incorrect_samples_pdf.close()\n",
    "    correct_samples_pdf.close()\n",
    "\n",
    "    # 输出处理过的模型信息\n",
    "    print(\"\\nProcessed Models:\")\n",
    "    for model_name, label_flag in processed_models:\n",
    "        print(f\"Model: {model_name}, Label Flag: {label_flag}\")\n",
    "\n",
    "    # 输出被跳过的文件信息\n",
    "    print(\"\\nSkipped Files:\")\n",
    "    for file, reason in skipped_files:\n",
    "        print(f\"File: {file}, Reason: {reason}\")\n"
   ],
   "id": "caac6d1a2215b4ff",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T09:14:56.782702Z",
     "start_time": "2024-12-20T09:12:09.925653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    process_models_to_pdfs(data_path)\n",
    "    print(\"PDFs saved successfully.\")"
   ],
   "id": "55281a6dc9aecf75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in data_path: ['en_torch_cnn_label_0.xlsx', 'en_torch_cnn_label_1.xlsx', 'en_torch_cnn_label_2.xlsx', 'en_torch_cnn_label_3.xlsx', 'torch_cnn_label_0.xlsx', 'torch_cnn_label_1.xlsx', 'torch_cnn_label_2.xlsx', 'torch_cnn_label_3.xlsx', 'torch_mlp_label_0.xlsx', 'torch_mlp_label_1.xlsx', 'torch_mlp_label_2.xlsx', 'torch_mlp_label_3.xlsx']\n",
      "Processing file: en_torch_cnn_label_0.xlsx, Parsed model_name: en_torch_cnn, label_flag: 0\n",
      "torch.Size([280000, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_tensor = torch.tensor(x_val).float().unsqueeze(1)\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val_tensor = torch.tensor(y_val).long()\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\3621929064.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside load_model: en_torch_cnn, label_flag=0\n",
      "Attempting to load model from: ./models/test//en_torch_cnn_0.pt\n",
      "Model en_torch_cnn loaded successfully.\n",
      "Model loaded: en_torch_cnn, label_flag=0\n",
      "Generating confusion matrix for model: en_torch_cnn, label_flag: 0\n",
      "Generating incorrect samples for model: en_torch_cnn, label_flag: 0\n",
      "Generating correct samples for model: en_torch_cnn, label_flag: 0\n",
      "Processing file: en_torch_cnn_label_1.xlsx, Parsed model_name: en_torch_cnn, label_flag: 1\n",
      "torch.Size([145600, 28, 28])\n",
      "Inside load_model: en_torch_cnn, label_flag=1\n",
      "Attempting to load model from: ./models/test//en_torch_cnn_1.pt\n",
      "Model en_torch_cnn loaded successfully.\n",
      "Model loaded: en_torch_cnn, label_flag=1\n",
      "Generating confusion matrix for model: en_torch_cnn, label_flag: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_tensor = torch.tensor(x_val).float().unsqueeze(1)\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val_tensor = torch.tensor(y_val).long()\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\3621929064.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating incorrect samples for model: en_torch_cnn, label_flag: 1\n",
      "Generating correct samples for model: en_torch_cnn, label_flag: 1\n",
      "Processing file: en_torch_cnn_label_2.xlsx, Parsed model_name: en_torch_cnn, label_flag: 2\n",
      "torch.Size([814255, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_tensor = torch.tensor(x_val).float().unsqueeze(1)\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val_tensor = torch.tensor(y_val).long()\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\3621929064.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside load_model: en_torch_cnn, label_flag=2\n",
      "Attempting to load model from: ./models/test//en_torch_cnn_2.pt\n",
      "Model en_torch_cnn loaded successfully.\n",
      "Model loaded: en_torch_cnn, label_flag=2\n",
      "Generating confusion matrix for model: en_torch_cnn, label_flag: 2\n",
      "Generating incorrect samples for model: en_torch_cnn, label_flag: 2\n",
      "Generating correct samples for model: en_torch_cnn, label_flag: 2\n",
      "Processing file: en_torch_cnn_label_3.xlsx, Parsed model_name: en_torch_cnn, label_flag: 3\n",
      "torch.Size([814255, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_tensor = torch.tensor(x_val).float().unsqueeze(1)\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val_tensor = torch.tensor(y_val).long()\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\3621929064.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside load_model: en_torch_cnn, label_flag=3\n",
      "Attempting to load model from: ./models/test//en_torch_cnn_3.pt\n",
      "Model en_torch_cnn loaded successfully.\n",
      "Model loaded: en_torch_cnn, label_flag=3\n",
      "Generating confusion matrix for model: en_torch_cnn, label_flag: 3\n",
      "Generating incorrect samples for model: en_torch_cnn, label_flag: 3\n",
      "Generating correct samples for model: en_torch_cnn, label_flag: 3\n",
      "Processing file: torch_cnn_label_0.xlsx, Parsed model_name: torch_cnn, label_flag: 0\n",
      "torch.Size([280000, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_tensor = torch.tensor(x_val).float().unsqueeze(1)\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val_tensor = torch.tensor(y_val).long()\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\3621929064.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside load_model: torch_cnn, label_flag=0\n",
      "Attempting to load model from: ./models/test//torch_cnn_0.pt\n",
      "Model torch_cnn loaded successfully.\n",
      "Model loaded: torch_cnn, label_flag=0\n",
      "Generating confusion matrix for model: torch_cnn, label_flag: 0\n",
      "Generating incorrect samples for model: torch_cnn, label_flag: 0\n",
      "Generating correct samples for model: torch_cnn, label_flag: 0\n",
      "Processing file: torch_cnn_label_1.xlsx, Parsed model_name: torch_cnn, label_flag: 1\n",
      "torch.Size([145600, 28, 28])\n",
      "Inside load_model: torch_cnn, label_flag=1\n",
      "Attempting to load model from: ./models/test//torch_cnn_1.pt\n",
      "Model torch_cnn loaded successfully.\n",
      "Model loaded: torch_cnn, label_flag=1\n",
      "Generating confusion matrix for model: torch_cnn, label_flag: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_tensor = torch.tensor(x_val).float().unsqueeze(1)\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val_tensor = torch.tensor(y_val).long()\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\3621929064.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating incorrect samples for model: torch_cnn, label_flag: 1\n",
      "Generating correct samples for model: torch_cnn, label_flag: 1\n",
      "Processing file: torch_cnn_label_2.xlsx, Parsed model_name: torch_cnn, label_flag: 2\n",
      "torch.Size([814255, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_tensor = torch.tensor(x_val).float().unsqueeze(1)\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val_tensor = torch.tensor(y_val).long()\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\3621929064.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside load_model: torch_cnn, label_flag=2\n",
      "Attempting to load model from: ./models/test//torch_cnn_2.pt\n",
      "Model torch_cnn loaded successfully.\n",
      "Model loaded: torch_cnn, label_flag=2\n",
      "Generating confusion matrix for model: torch_cnn, label_flag: 2\n",
      "Generating incorrect samples for model: torch_cnn, label_flag: 2\n",
      "Generating correct samples for model: torch_cnn, label_flag: 2\n",
      "Processing file: torch_cnn_label_3.xlsx, Parsed model_name: torch_cnn, label_flag: 3\n",
      "torch.Size([814255, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_tensor = torch.tensor(x_val).float().unsqueeze(1)\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val_tensor = torch.tensor(y_val).long()\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\3621929064.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside load_model: torch_cnn, label_flag=3\n",
      "Attempting to load model from: ./models/test//torch_cnn_3.pt\n",
      "Model torch_cnn loaded successfully.\n",
      "Model loaded: torch_cnn, label_flag=3\n",
      "Generating confusion matrix for model: torch_cnn, label_flag: 3\n",
      "Generating incorrect samples for model: torch_cnn, label_flag: 3\n",
      "Generating correct samples for model: torch_cnn, label_flag: 3\n",
      "Processing file: torch_mlp_label_0.xlsx, Parsed model_name: torch_mlp, label_flag: 0\n",
      "torch.Size([280000, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_tensor = torch.tensor(x_val).float().unsqueeze(1)\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val_tensor = torch.tensor(y_val).long()\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\3621929064.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside load_model: torch_mlp, label_flag=0\n",
      "Attempting to load model from: ./models/test//torch_mlp_0.pt\n",
      "Model torch_mlp loaded successfully.\n",
      "Model loaded: torch_mlp, label_flag=0\n",
      "Generating confusion matrix for model: torch_mlp, label_flag: 0\n",
      "Generating incorrect samples for model: torch_mlp, label_flag: 0\n",
      "Generating correct samples for model: torch_mlp, label_flag: 0\n",
      "Processing file: torch_mlp_label_1.xlsx, Parsed model_name: torch_mlp, label_flag: 1\n",
      "torch.Size([145600, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_tensor = torch.tensor(x_val).float().unsqueeze(1)\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val_tensor = torch.tensor(y_val).long()\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\3621929064.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside load_model: torch_mlp, label_flag=1\n",
      "Attempting to load model from: ./models/test//torch_mlp_1.pt\n",
      "Model torch_mlp loaded successfully.\n",
      "Model loaded: torch_mlp, label_flag=1\n",
      "Generating confusion matrix for model: torch_mlp, label_flag: 1\n",
      "Generating incorrect samples for model: torch_mlp, label_flag: 1\n",
      "Generating correct samples for model: torch_mlp, label_flag: 1\n",
      "Processing file: torch_mlp_label_2.xlsx, Parsed model_name: torch_mlp, label_flag: 2\n",
      "torch.Size([814255, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_tensor = torch.tensor(x_val).float().unsqueeze(1)\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val_tensor = torch.tensor(y_val).long()\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\3621929064.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside load_model: torch_mlp, label_flag=2\n",
      "Attempting to load model from: ./models/test//torch_mlp_2.pt\n",
      "Model torch_mlp loaded successfully.\n",
      "Model loaded: torch_mlp, label_flag=2\n",
      "Generating confusion matrix for model: torch_mlp, label_flag: 2\n",
      "Generating incorrect samples for model: torch_mlp, label_flag: 2\n",
      "Generating correct samples for model: torch_mlp, label_flag: 2\n",
      "Processing file: torch_mlp_label_3.xlsx, Parsed model_name: torch_mlp, label_flag: 3\n",
      "torch.Size([814255, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_tensor = torch.tensor(x_val).float().unsqueeze(1)\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\2463288146.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val_tensor = torch.tensor(y_val).long()\n",
      "C:\\Users\\20971\\AppData\\Local\\Temp\\ipykernel_17820\\3621929064.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside load_model: torch_mlp, label_flag=3\n",
      "Attempting to load model from: ./models/test//torch_mlp_3.pt\n",
      "Model torch_mlp loaded successfully.\n",
      "Model loaded: torch_mlp, label_flag=3\n",
      "Generating confusion matrix for model: torch_mlp, label_flag: 3\n",
      "Generating incorrect samples for model: torch_mlp, label_flag: 3\n",
      "Generating correct samples for model: torch_mlp, label_flag: 3\n",
      "\n",
      "Processed Models:\n",
      "Model: en_torch_cnn, Label Flag: 0\n",
      "Model: en_torch_cnn, Label Flag: 1\n",
      "Model: en_torch_cnn, Label Flag: 2\n",
      "Model: en_torch_cnn, Label Flag: 3\n",
      "Model: torch_cnn, Label Flag: 0\n",
      "Model: torch_cnn, Label Flag: 1\n",
      "Model: torch_cnn, Label Flag: 2\n",
      "Model: torch_cnn, Label Flag: 3\n",
      "Model: torch_mlp, Label Flag: 0\n",
      "Model: torch_mlp, Label Flag: 1\n",
      "Model: torch_mlp, Label Flag: 2\n",
      "Model: torch_mlp, Label Flag: 3\n",
      "\n",
      "Skipped Files:\n",
      "PDFs saved successfully.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T09:48:11.584144Z",
     "start_time": "2024-12-20T09:48:10.934114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "def split_pdf(input_pdf_path, output_dir, filename_format=\"page_{:03d}.pdf\"):\n",
    "    \"\"\"\n",
    "    将 PDF 的每一页保存为单独的 PDF 文件。\n",
    "\n",
    "    参数：\n",
    "    - input_pdf_path (str): 输入 PDF 文件的路径。\n",
    "    - output_dir (str): 输出文件的保存路径。\n",
    "    - filename_format (str): 每个输出 PDF 的文件名格式，例如 \"page_{:03d}.pdf\"。\n",
    "    \"\"\"\n",
    "    # 检查输出目录是否存在，不存在则创建\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 读取输入 PDF 文件\n",
    "    reader = PdfReader(input_pdf_path)\n",
    "    total_pages = len(reader.pages)\n",
    "\n",
    "    print(f\"正在处理 {input_pdf_path}，共 {total_pages} 页\")\n",
    "\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        writer = PdfWriter()\n",
    "        writer.add_page(page)\n",
    "\n",
    "        # 生成每页的文件名\n",
    "        output_filename = filename_format.format(i + 1)\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "        # 保存单页 PDF\n",
    "        with open(output_path, \"wb\") as output_pdf:\n",
    "            writer.write(output_pdf)\n",
    "\n",
    "        print(f\"保存页面 {i + 1} 为文件：{output_path}\")\n",
    "\n",
    "    print(\"PDF 分页完成！\")\n",
    "\n",
    "# 示例用法\n",
    "input_pdf = \"./incorrect_samples.pdf\"  # 替换为你的 PDF 文件路径\n",
    "output_directory = \"./output\"  # 替换为保存 PDF 的文件夹路径\n",
    "split_pdf(input_pdf, output_directory, filename_format=\"incorrect_samples_{:02d}.pdf\")\n"
   ],
   "id": "55c5c2e76ad7980",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理 ./incorrect_samples.pdf，共 12 页\n",
      "保存页面 1 为文件：./output\\incorrect_samples_01.pdf\n",
      "保存页面 2 为文件：./output\\incorrect_samples_02.pdf\n",
      "保存页面 3 为文件：./output\\incorrect_samples_03.pdf\n",
      "保存页面 4 为文件：./output\\incorrect_samples_04.pdf\n",
      "保存页面 5 为文件：./output\\incorrect_samples_05.pdf\n",
      "保存页面 6 为文件：./output\\incorrect_samples_06.pdf\n",
      "保存页面 7 为文件：./output\\incorrect_samples_07.pdf\n",
      "保存页面 8 为文件：./output\\incorrect_samples_08.pdf\n",
      "保存页面 9 为文件：./output\\incorrect_samples_09.pdf\n",
      "保存页面 10 为文件：./output\\incorrect_samples_10.pdf\n",
      "保存页面 11 为文件：./output\\incorrect_samples_11.pdf\n",
      "保存页面 12 为文件：./output\\incorrect_samples_12.pdf\n",
      "PDF 分页完成！\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3c089a3c92713562"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
